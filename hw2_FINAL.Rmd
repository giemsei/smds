---
title: "GroupG_HM2"
author: "Michele Alessi, Gianmarco Alessio, Gabriele Codega, Leonardo Musini"
date: "2022-11-21"
output:
  html_document:
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## FSDS - Chapter 3

### Ex 3.12

Simulate a random sampling from a normal population distribution with several $n$ values to illustrate the law of large numbers.

**Solution**

We want to simulate a sample of $n=10$,$n=1000$,$n=1000000$ (the number of datasets to be simulated) and we generate an outcome ($\overline{y}$) with a normal population distribution of $mean=50$ and $var=10$. We took the mean of this outcomes for each 3 cases and to prove the law of large numbers we used *the convergence in probability* of $\overline{y}$ to $\mu$ for $n\to\infty$.

```{r}
set.seed(11)
mean(rnorm(10,50,10));
mean(rnorm(10^3, 50, 10));
mean(rnorm(10^7,50,10))
```

As we can see the mean converge to $\mu=50$ as $n$ increase.


### Ex 3.18
Sunshine City, which attracts primarily retired people, has 90,000 residents with a mean age
of 72 years and a standard deviation of 12 years. The age distribution is skewed to the left. A
random sample of 100 residents of Sunshine City has $\overline y =70$ and $s = 11$.

(a) Describe the center and spread of the (i) population distribution, (ii) sample data distribution. What shape does the sample data distribution probably have? Why?

(b) Find the center and spread of the sampling distribution of $\overline Y$ for $n = 100$. What shape
does it have and what does it describe?

(c) Explain why it would not be unusual to sample a person of age 60 in Sunshine City, but
it would be highly unusual for the sample mean to be 60, for a random sample of 100
residents.

(d) Describe the sampling distribution of $\overline Y$ : (i) for a random sample of size $n = 1$; (ii) if you
sample all 90,000 residents.

**Solution**

(a)
Due to the fact the distribution is skewed to the left, the mean has a lesser value than the median and the mode, in particular $mean<median<mode$. Both for the population and the sample, the means of values respectively 72 and 70 and  the two standard deviations 12 and 11 indicate that the residents are quite old. 
The sample data distribution is probably shaped as the population distribution, so skewed to the left, because the sample is a subset of the population and greater is the size $n$ of the sample, greater will be the similarity with the population distribution.

(b)
The center and the spread of a sampling distribution of the sample means $\overline{Y}$ are defined as:

\begin{equation}
E[\overline{Y}]=\mu \qquad SE(\overline{Y})=\frac{\sigma}{\sqrt(n)}
\end{equation}

In which $\mu$ and $\sigma$ are respectively the mean and standard deviation of the sample. If we calculate the center and the spread with the corresponding values we'll obtain $E[\overline{Y}]=70$ and $SE(\overline{Y})=1.1$.
The simulation of the sampling distribution for a sample of $n=100$ is:

```{r, echo=TRUE}

set.seed(1234)

M <- 900; n <- 100
sample_means <- rep(NA, M)
for (j in 1:M){
  sample_means[j] <- mean(rnorm(n, 70, 11))
}

mean(sample_means)
sd(sample_means)

hist(sample_means, main="", xlab="Sample Mean")

```

We can observe that the simulated values of the center and the spread are very close to those calculated theoretically.
The sampling distribution is bell-shaped as we expect due to the Central Limit Theorem and it describes the probability of the statistic falling within a certain distance of the population parameter that it estimates, in this case the sample means.

(c)
It isn't unusual to sample a person with age 60 because it falls within 1 standard deviation of the population distribution with mean 72 and the population distribution is skewed to the left, so to lesser ages. But it will be unusual that the mean of a sample distribution of 100 residents is 60 because as we can see from the sampling distribution, that is the probability distribution for the possible values of the sample mean, we observe that the mean of the sample means is 70 with a standard error of 1.1.
Moreover, the sample distribution more closely resembles the population distribution as $n$ increases.

(d)
```{r, echo=TRUE}

set.seed(1234)

M <- 1000; n <- 1
sample_means <- rep(NA, M)
for (j in 1:M){
  sample_means[j] <- mean(rnorm(n, 70, 11))
}
mean(sample_means)
sd(sample_means)
hist(sample_means, main="", xlab="Sample Mean")

```

The sampling distribution for samples of size $n=1$ shows a normal distribution, as we expect, and a mean very close to the mean of the $n=100$ sample distribution. The standard deviation $\sigma=10$ shows that with a probability close to 1 that the sample means fall roughly between 40 and 100.
```{r, echo=TRUE}

set.seed(1234)

M <- 1000; n <- 90000
sample_means <- rep(NA, M)
for (j in 1:M){
  sample_means[j] <- mean(rnorm(n, 72, 12))
}

mean(sample_means)
sd(sample_means)
hist(sample_means, main="", xlab="Sample Mean")

```

Also the sampling distribution of size $n=90000$ has a normal distribution, the mean is close to the mean of the population distribution, but in this case the standard deviation is small. This means that the means of the population falls between a pretty shorter interval compared to the previuos one.


## FSDS - Chapter 4
### Ex 4.4
For the Students data file (Exercise 1.2 in Chapter 1) and corresponding population, find the
ML estimate of the population proportion believing in life after death. Construct a Wald 95%
confidence interval, using its formula (4.8). Interpret.

**Solution**

The Students data file shows responses of a class of 60 students to a questionnaire that asked about some topics. The question about life after death has 3 answers: yes, no, undecided.
We can easily observe how many of them answered yes:
```{r, echo=TRUE}

data <- read.table("Students.dat.txt", header = TRUE)
tot <- data[, "life"]
c <- tot[tot<2]
length(c)

```
The proportion of population believing in life after death could be estimate using a binomial distribution function; indeed the proportion is the probability of students believing in afterlife.
We can use the logarithm of the binomial distribution function to estimate the probability with the ML method:

\begin{equation}
l(p)=\log(\binom{n}{k}p^k(1-p)^{n-k})=\log(\binom{n}{k})+k\log(p)+(n-k)\log(1-p)
\end{equation}

With $n=60$ the population and $k=31$ students who believe in afterlife.
Now we calculate the estimation of $p$ using the score function $U(p)$:

\begin{equation}
U(p)=\frac{\partial l(p)}{\partial p}=\frac{k}{p}-\frac{n-k}{1-p}=0
\end{equation}

Solving the equation for $p$ it gives the following estimation: $\hat p=0.517$.
We have to check if we have a maximum of the $l(p)$ function using the observed information function $J(p)$:

\begin{equation}
J(\hat p)=-\frac{\partial^2 l(p)}{\partial p^2}=-(-\frac{k}{p^2}-\frac{n-k}{(1-p)^2})= \frac{k}{p}+\frac{n-k}{(1-p)^2}
\end{equation}

We have a maximum because $J(\hat p)>0$.
We know that the sample proportion $\hat p$ is the sample mean for binary data, the sampling distribution of the estimated proportion is approximately normal. We can construct a Wald 95%
confidence interval using the following formula:

\begin{equation}
\hat p \pm z_{\alpha/2}\sqrt\frac{\hat p(1-\hat p)}{n}
\end{equation}

In which $z_{\alpha/2}$ is the standard normal percentiles for $\alpha=0.05$.
```{r, echo=TRUE}
z <- qnorm(0.95)
p <- 0.517
n <- 60
int <- z*sqrt(p*(1-p)/n)
int
```
So the estimated confidence interval is:

\begin{equation}
0.517 \pm 0.106
\end{equation}

We can see that the estimation of the percentage of students that believe in a life after death is above $50\%$, but in an interval that can accept values even under $50\%$. We can conclude that roughly between $40\%$ and $60\%$ the population has a belief of life after death.


### Ex 4.48

For a simple random sample of $n$ subjects, explain why it is about $95\%$ likely that the sample proportion has error no more that $1/\sqrt{n}$ in estimating the population proportion. (*Hint*: To show this "$1/\sqrt{n}$ rule", find two standard errors when $\pi=0.50$, and explain how this compares to two standard errors at other values of $\pi$). Using this result, show that $n=1/M^2$ is a safe sample size for estimating a proportion to within $M$ with $95\%$ confidence.

**Solution**

The sampling distribution of the simple random sample $\hat{\pi}$ is approximately normal for large $n$. So the Pivotal Quantity

$$Z=\frac{\hat{\pi}-\pi}{\sqrt{\frac{\pi(1-\pi)}{n}}}$$

has an approximate standard normal distribution. Bounding this between standard normal percentiles, to construct a CI for $\pi$ after observing the simple random sample $\hat{\pi}$.

$$\hat{\pi} -\pi=\pm z_{\alpha/2}\sqrt{\frac{\pi(1-\pi)}{n}}$$
From here we use the simple approximate formula for large $n$ (Wald confidence interval) to easily compute the CI:

$$\hat{\pi}\pm z_{\alpha/2} \sqrt{\frac{\hat{\pi}(1-\hat{\pi})}{n}}$$
So from the exercise we assume the confidence level of $1-\alpha=0.95$ and since $95\%$ of the standard normal distribution falls between $-1.96$ and $1.96$ we have:

$$\hat{\pi}\pm 1.96 \sqrt{\frac{\hat{\pi}(1-\hat{\pi})}{n}}$$
We can observe $1.96\sim2.00$ (the exercise sugestion) and $\sqrt{\hat{\pi}(1-\hat{\pi})}\sim 0.50$ (considering $\hat{\pi}=0.50$) so:

$$\hat{\pi}\pm \frac{1}{\sqrt{n}}$$
And with a simple simulation we can verify that:

```{r}
set.seed(11)
simulation <- 100
size<-100
thresholdrule <- 1/sqrt(size)

standarderror <- function(phi){
y<-rbinom(simulation,size,phi)
p<-y/size
sd <- sqrt(p*(1-p)/size)
return (sd)
}

#standarderror(0.50)
#standarderror(0.10)


all(standarderror(0.50)<thresholdrule)
all(standarderror(0.10)<thresholdrule)

```

Then if we consider the sample size $n=1/M^2$ and at the same time we want $n$ to be able to estimate $\pi$. We have

$$\hat{\pi}\pm M$$

## FSDS - Chapter 5

### Ex 5.2
When a government does not have enough money to pay for the services that it provides, it can raise taxes or it can reduce services. When the Florida Poll asked a random sample of 1200 Floridians which they preferred, 52% (624 of the 1200) chose raise taxes and 48% chose reduce services. Let $\pi$ denote the population proportion of Floridians who would choose raising taxes. Analyze whether this is a minority of the population ($\pi < 0.50$) or a majority ($\pi > 0.50$) by testing $H_0 ∶ \pi = 0.50$ against $H_a: π \ne 0.50$. Interpret the P-value.Is it appropriate to “accept $H_0$? Why or whynot?

**Solution**


### Ex 5.16
An experiment used a sample of college students to investigate whether cell phone use impairs
drivers’ reaction times. On a machine that simulated driving situations, at irregular periods
a target flashed red or green. Participants were instructed to press a brake button as soon as
possible when they detected a red light. Under the cell phone condition, each student carried
out a conversation on a cell phone with someone in a separate room. In the control condition,
the same students listened to a radio broadcast. The CellPhone data file records the students’
mean response times (in milliseconds) over several trials for each condition, {$y_{i1}$ for the cell
phone condition and {$y_{i2}$} for control.

(a) The comparisons of means or proportions in this chapter assume independent samples for
the two groups. Explain why the samples for these two conditions are dependent rather
than independent.

(b) To compare $\mu_1$ and $\mu_2$, you can use {$d_i = y_{i1} − y_{i2}$, $i = 1, . . . , n$}, here with $n = 8$. Specify
the parameter $\mu_d$ and $H_0$ for doing this, and explain why $\mu_d = \mu_1 − \mu_2$.

(c) State the assumptions and test statistic, explain why it has a t distribution with $df = n−1$.
Report the P-value with two-sided $H_a$, and interpret. (The test is called a matched-pairs
t test. Matched-pairs analyses also are possible with confidence intervals, as Section 4.4.3
did in comparing weights of anorexic girls before and after a period of treatment by
analyzing the mean difference in weights.)

**Solution**

(a)
In this case the samples for the phone and for the radio conditions are dependent because they are the reaction times mean of the same 8 students. So each pair of data is influenced by the same reaction capability without condition of the same student. The reaction times of phone sample are shifted compared to the control sample.

(b)
We can easily calculate $\mu_d$ as the mean of the samples difference:

\begin{equation}
\mu_d =\frac{1}{n}\sideset{}{_{i=1}^n}\sum (y_{1i}-y_{2i}) 
\end{equation}

We can also choose the null hypothesis as the assumption that the use of phone don't influence the reaction times of the students: $H_0:\mu_d=0$.
We can easily show $\mu_d=\mu_1-\mu_2$ with the following demonstration:

\begin{equation}
\mu_d=\frac{1}{n}\sideset{}{_{i=1}^n}\sum (y_{1i}-y_{2i})=\frac{\sideset{}{_{i=1}^n}\sum y_{1i}}{n}- \frac{\sideset{}{_{i=1}^n}\sum y_{2i}}{n}=\mu_1-\mu_2
\end{equation}

(c)
The principal assumption is that the two samples means  are normally distributed, so we can assume that even the differences have a normal distribution.
In point $b.$ we chose the null hypothesis as $H_o:\mu_d=0$, so if we want a two-sided hypothesis test we can choose the alternative hypothesis as $H_a:\mu_d \neq 0$, that is the assumption that the use of phone change the reaction times of the students.
The test statistic for a significance test for a mean is in this case is a $t$ distribution:

\begin{equation}
T=\frac{\overline Y}{SE} \qquad with \quad SE=\frac{s}{\sqrt n}
\end{equation}

that, for observations from a normal distribution, has has a sampling distribution over the entire real line that is slightly more spread out than the standard normal distribution.
The degrees of freedom are $df=n-1$ because, even if we are comparing two sample of means, we are doing a significance test for the difference of the means $d_i$, so an inference about a single population mean. 

```{r, echo=TRUE}

data <- read.table("CellPhone.dat.txt", header = TRUE)
phone <- data[,"phone"]
control <- data[,"control"]
difference <- phone-control
t.test(difference)
```

As we can see the p-value has a very small value that indicates a low probability that the null hypothesis is true.
We can conclude that the use of phones modifies the reaction time.


